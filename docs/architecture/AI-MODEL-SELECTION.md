# FlowSight AI 模型选择方案

> 文档版本: 1.0  
> 最后更新: 2025-01-07

---

## 1. 需求分析

### 1.1 我们需要 AI 做什么？

FlowSight AI 的核心任务是**弥补静态分析无法 100% 确定的部分**：

| 任务 | 输入 | 输出 |
|------|------|------|
| 函数指针目标预测 | 代码上下文 + 未知的 `fp()` | 最可能的目标函数列表 |
| callback 触发时机 | handler 函数定义 | 何时被调用的描述 |
| 异步模式识别 | 代码片段 | 识别出的异步模式类型 |

### 1.2 部署要求

| 要求 | 说明 |
|------|------|
| **本地部署** | 下载 IDE 即自带 AI，无需联网 |
| **离线使用** | 完全离线，不依赖云服务 |
| **资源限制** | 普通电脑可运行（8-16GB 内存） |
| **跨平台** | Windows / macOS / Linux |

### 1.3 模型大小目标

- 量化后模型文件：**2-4GB**
- 最低内存需求：**8GB**
- 推荐内存需求：**16GB**

---

## 2. 模型调研

### 2.1 代码领域开源模型对比

| 模型 | 参数量 | HumanEval | 许可证 | C/C++ 能力 |
|------|--------|-----------|--------|------------|
| **DeepSeek-Coder** | 1.3B/6.7B/33B | 73.8% (6.7B) | MIT | ⭐⭐⭐⭐⭐ |
| CodeLlama | 7B/13B/34B | 53.7% (34B) | Llama 2 | ⭐⭐⭐⭐ |
| StarCoder2 | 3B/7B/15B | 46.0% (15B) | OpenRAIL | ⭐⭐⭐⭐ |
| Qwen2.5-Coder | 1.5B/7B/32B | 新发布 | Apache 2.0 | ⭐⭐⭐⭐ |
| CodeGemma | 2B/7B | - | Gemma | ⭐⭐⭐ |
| Phi-3 | 3.8B | 通用模型 | MIT | ⭐⭐⭐ |

### 2.2 基准测试说明

- **HumanEval**: 代码生成基准测试，164 道编程题
- **MBPP**: 代码理解和生成基准
- **DS-1000**: 数据科学代码基准

---

## 3. 最终选择

### 3.1 主力模型：DeepSeek-Coder-6.7B

```
┌─────────────────────────────────────────────────────────────────────────┐
│                     选择 DeepSeek-Coder-6.7B 的理由                      │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  理由1：代码能力最强                                                    │
│  ═══════════════════════                                                │
│  • HumanEval 73.8%，超过 CodeLlama-34B (53.7%)                         │
│  • 参数量只有 CodeLlama-34B 的 1/5，但效果更好                         │
│  • 在 C/C++ 代码理解上表现尤其出色                                     │
│  • 支持 16K 上下文长度，可分析大段代码                                 │
│                                                                          │
│  理由2：大小适中                                                        │
│  ═══════════════════                                                    │
│  • 6.7B 参数，原始约 13GB                                              │
│  • INT4 量化后约 4GB                                                   │
│  • 16GB 内存可流畅运行                                                 │
│  • 8GB 内存可运行（较慢）                                              │
│                                                                          │
│  理由3：完全开源                                                        │
│  ═══════════════════                                                    │
│  • MIT 许可证                                                           │
│  • 可商用，无限制                                                       │
│  • 可自由修改和分发                                                     │
│                                                                          │
│  理由4：蒸馏方案成熟                                                    │
│  ═══════════════════════                                                │
│  • 同系列有 1.3B 小版本，架构兼容                                      │
│  • 可从 6.7B 蒸馏到 1.3B                                               │
│  • 有丰富的微调和蒸馏教程                                               │
│                                                                          │
│  理由5：社区活跃                                                        │
│  ═══════════════════                                                    │
│  • GitHub 6000+ Stars                                                   │
│  • 大量使用案例和经验分享                                               │
│  • 持续更新维护                                                         │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 3.2 蒸馏目标：DeepSeek-Coder-1.3B

| 指标 | 6.7B (教师) | 1.3B (学生) |
|------|-------------|-------------|
| 参数量 | 6.7B | 1.3B |
| 原始大小 | ~13GB | ~2.6GB |
| INT4 量化 | ~4GB | ~1GB |
| 内存需求 | 16GB | 8GB |
| 推理速度 (CPU) | 3-5 tok/s | 10-15 tok/s |

### 3.3 备选方案

如果 DeepSeek-Coder 在实际测试中效果不理想：

| 优先级 | 备选模型 | 切换理由 |
|--------|----------|----------|
| 1 | Qwen2.5-Coder-7B | 最新发布，可能更强 |
| 2 | StarCoder2-7B | 80+ 语言支持 |
| 3 | WizardCoder-15B | 指令跟随能力强 |

---

## 4. 技术实现

### 4.1 训练流程

```
Step 1: 数据准备
────────────────
• 收集开源项目代码 (Linux 内核、Android、常见库)
• 标注代码执行流 (函数指针目标、callback 时机)
• 构建问答数据集

Step 2: 微调教师模型
────────────────────
• 基础模型: DeepSeek-Coder-6.7B
• 微调方法: LoRA / QLoRA
• 训练任务: 函数指针预测、异步模式识别
• 评估指标: 准确率、召回率

Step 3: 知识蒸馏
────────────────
• 教师模型: 微调后的 DeepSeek-Coder-6.7B
• 学生模型: DeepSeek-Coder-1.3B
• 蒸馏方法: 软标签蒸馏
• 目标: 保留 90%+ 的任务性能

Step 4: 量化优化
────────────────
• 量化方法: GPTQ / AWQ / GGUF
• 量化精度: INT4 (主推) / INT8 (备选)
• 目标大小: 1-2GB
```

### 4.2 部署架构

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    FlowSight IDE                                         │
│  ┌───────────────────────────────────────────────────────────────────┐  │
│  │  Rust 后端                                                         │  │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐               │  │
│  │  │ 静态分析    │  │ flowsight-ai │  │ 知识库      │               │  │
│  │  │ 引擎        │  │ (llama.cpp)  │  │             │               │  │
│  │  └─────────────┘  └─────────────┘  └─────────────┘               │  │
│  │         │                │                │                        │  │
│  │         └────────────────┴────────────────┘                        │  │
│  │                          │                                          │  │
│  │                    分析结果                                         │  │
│  └───────────────────────────────────────────────────────────────────┘  │
│                              │                                           │
│                              ▼                                           │
│  ┌───────────────────────────────────────────────────────────────────┐  │
│  │  前端 (React + React Flow)                                         │  │
│  │  • 执行流可视化                                                    │  │
│  │  • AI 预测展示 (🤖 标记)                                          │  │
│  │  • 用户确认/否定                                                   │  │
│  └───────────────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────────────┘
```

### 4.3 推理引擎选择

| 引擎 | 优势 | 劣势 |
|------|------|------|
| **llama.cpp** | 跨平台、CPU 优化、GGUF 格式 | C++ 需要绑定 |
| onnxruntime | 通用、多后端 | 模型转换复杂 |
| candle | 纯 Rust | 生态较新 |

**选择: llama.cpp** (通过 llama-cpp-rs Rust 绑定)

---

## 5. 模型分发

### 5.1 分发方式

| 方式 | 适用场景 |
|------|----------|
| **按需下载** (推荐) | IDE 安装包小，首次使用时下载模型 |
| 内置模型 | 安装包大，但开箱即用 |

### 5.2 下载源

- 主要: GitHub Releases
- 备用: HuggingFace Hub
- 国内: 自建 CDN 或镜像

### 5.3 模型文件

```
~/.flowsight/models/
├── flowsight-code-v1.gguf    # 主模型 (~1-2GB)
├── flowsight-code-v1.sha256  # 校验文件
└── version.json              # 版本信息
```

---

## 6. 风险与应对

| 风险 | 可能性 | 影响 | 应对措施 |
|------|--------|------|----------|
| 模型效果不达预期 | 中 | 高 | 切换备选模型，增加训练数据 |
| 本地推理太慢 | 中 | 中 | 优化量化，支持 GPU 加速 |
| 模型文件太大 | 低 | 中 | 进一步量化，或提供多版本 |
| 跨平台兼容问题 | 低 | 中 | 充分测试三平台 |

---

## 7. 时间计划

| 阶段 | 内容 | 时间 |
|------|------|------|
| 数据收集 | 收集代码 + 标注执行流 | 4 周 |
| 模型微调 | LoRA 微调 DeepSeek-Coder-6.7B | 2 周 |
| 知识蒸馏 | 蒸馏到 1.3B | 2 周 |
| 量化优化 | GGUF 量化 | 1 周 |
| 集成测试 | 集成到 IDE + 测试 | 3 周 |
| **总计** | | **12 周** |

---

## 附录：参考资源

- [DeepSeek-Coder GitHub](https://github.com/deepseek-ai/DeepSeek-Coder)
- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)
- [llama-cpp-rs](https://github.com/rustformers/llama-cpp-rs)
- [PEFT (LoRA)](https://github.com/huggingface/peft)

